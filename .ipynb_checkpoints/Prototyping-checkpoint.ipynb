{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4b83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import PyPDF2 as pdf\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import openai\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "api_key_file = open(\"OPENAI_API_KEY\", \"r\")\n",
    "openai.api_key = api_key_file.read()\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "def embed(x):\n",
    "    '''embed with openai'''\n",
    "    return get_embedding(x, engine=\"text-embedding-ada-002\")\n",
    "\n",
    "def reset_data():\n",
    "    '''clear all data from directories and json history'''\n",
    "    data_json = {\"stored_pdfs\":{}\n",
    "    }\n",
    "    json_object = json.dumps(data_json, indent=4)\n",
    "    with open(\"Prototype/data_json.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)  \n",
    "    manual_vectors = pd.DataFrame({\"pdf_id\":[],\"pdf_name\":[],\"avg_embedding\":[]})\n",
    "    manual_vectors.to_pickle('Prototype/vectors/manual_vectors.csv')\n",
    "    folder = \"Prototype/vectors/PageVDs\"\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def load_pdfs(reset=False):\n",
    "    '''Loads pdfs that have not been loaded into the dataframes. Set reset to True to wipe all data and start again'''\n",
    "    if reset == True:\n",
    "        reset_data()\n",
    "    with open(\"Prototype/data_json.json\", 'r') as openfile:\n",
    "        data_json = json.load(openfile)\n",
    "    pdf_files = os.listdir(\"Prototype/pdf_files\")\n",
    "    max_id = len(data_json.keys())\n",
    "    manual_vectors = pd.read_pickle('Prototype/vectors/manual_vectors.csv')\n",
    "    count = 0\n",
    "    for file in pdf_files:\n",
    "        if file == \".DS_Store\":\n",
    "            continue\n",
    "        \n",
    "        if file not in data_json[\"stored_pdfs\"].values():\n",
    "            count+=1\n",
    "            cur_id = len(data_json[\"stored_pdfs\"].keys())\n",
    "            data_json[\"stored_pdfs\"][cur_id] = file\n",
    "            # PYPDF Part\n",
    "            cur_pdf_text = []\n",
    "            print(f\"processing {file}\")\n",
    "            with open(\"Prototype/pdf_files/\" + file,\"rb\") as pdf_file:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                for j in range(0, len(reader.pages)):\n",
    "                    cur_string = reader.pages[j].extract_text()\n",
    "                    cur_pdf_text.append(\"|start of page {} \".format(j+1)+cur_string + \" end of page {}|\".format(j+1))\n",
    "            page_df = pd.DataFrame({\"pg_no\":list(range(1,len(cur_pdf_text)+1)),\"pg_text\":cur_pdf_text,\"pdf_id\":np.full(len(cur_pdf_text),cur_id)})\n",
    "            print(\"embedding vectors...\")\n",
    "            max_count = len(page_df[\"pg_text\"])\n",
    "            \n",
    "            with \n",
    "            \n",
    "            display(f)\n",
    "            def embed_load(x):\n",
    "                f.value+=1\n",
    "                return embed(x)\n",
    "            page_df[\"pg_embedding\"] = page_df[\"pg_text\"].apply(embed_load)\n",
    "            clear_output()\n",
    "            print(f\"{file} complete!\")\n",
    "            manual_vectors = manual_vectors.append({\"pdf_id\":cur_id,\"pdf_name\":file,\"avg_embedding\":np.array(list(page_df[\"pg_embedding\"])).mean(axis=0)},ignore_index=True)\n",
    "            page_df.to_pickle(f'Prototype/vectors/PageVDs/{cur_id}.csv')\n",
    "            \n",
    "        else:\n",
    "            print(f\"{file} is a duplicate file\")\n",
    "    clear_output()\n",
    "    manual_vectors.to_pickle('Prototype/vectors/manual_vectors.csv')\n",
    "    json_object = json.dumps(data_json, indent=4)\n",
    "    with open(\"Prototype/data_json.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "        \n",
    "    if count == 0:\n",
    "        print(\"No new files detected.\")\n",
    "    elif count == 1:\n",
    "        print(f\"Added {count} new file to the database.\")\n",
    "    else:\n",
    "        print(f\"Added {count} new files to the database.\")\n",
    "    \n",
    "def get_suitable_documents(prompt, k_docs=8, details=False):\n",
    "    '''Queries into the vector database to find the most suitable documents based on the prompt'''\n",
    "    embedded_prompt = embed(prompt)\n",
    "    manual_df = pd.read_pickle('Prototype/vectors/manual_vectors.csv')\n",
    "    manual_df[\"cos\"] = manual_df[\"avg_embedding\"].apply(lambda x: x@embedded_prompt)\n",
    "    manual_df = manual_df.sort_values(by=\"cos\",ascending=False)\n",
    "    if details == True:\n",
    "        print(\"looking at:\",list(manual_df[\"pdf_name\"])[0:k_docs])\n",
    "    return list(manual_df[\"pdf_id\"][0:k_docs])\n",
    "    \n",
    "    \n",
    "def get_page_text(pdf_id,pg_no):\n",
    "    '''Queries into the database to get the text of a pdf given a page'''\n",
    "    try:\n",
    "        page_df = pd.read_pickle(f\"Prototype/vectors/PageVDs/{int(pdf_id)}.csv\")\n",
    "        return page_df[page_df[\"pg_no\"] == pg_no].iloc[0][\"pg_text\"]\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "    \n",
    "def get_suitable_text(prompt,k_docs=8, k_pages=10, details=False, threshold = 0.7):\n",
    "    '''Queries into the vector database to find the most suitable context text based on the prompt'''\n",
    "    with open(\"Prototype/data_json.json\", 'r') as openfile:\n",
    "        data_json = json.load(openfile)\n",
    "    #Find suitable manuals\n",
    "    embedded_prompt = embed(prompt)\n",
    "    doc_ids = get_suitable_documents(prompt,k_docs,details)\n",
    "    relevant_df = pd.DataFrame()\n",
    "    for doc_id in doc_ids:\n",
    "        relevant_df = relevant_df.append(pd.read_pickle(f\"Prototype/vectors/PageVDs/{int(doc_id)}.csv\"),ignore_index=True)\n",
    "    #Merge Documents\n",
    "    relevant_df[\"pg_cos\"] = relevant_df[\"pg_embedding\"].apply(lambda x: np.array(x)@embedded_prompt)\n",
    "    ranked_filtered_df = relevant_df.sort_values(by=\"pg_cos\",ascending=False)\n",
    "    idx_pg_pairs = sorted(list(zip(ranked_filtered_df['pdf_id'][0:k_pages],ranked_filtered_df['pg_no'][0:k_pages])))\n",
    "    idx_pg_dict = {}\n",
    "    \n",
    "    for key, value in idx_pg_pairs:\n",
    "        idx_pg_dict.setdefault(key, []).append(value)\n",
    "        \n",
    "    for key,value in idx_pg_dict.items():\n",
    "        page_list = value\n",
    "        new_list = []\n",
    "        for i,page in enumerate(page_list):\n",
    "            if i == len(page_list)-1:\n",
    "                new_list.append(page)\n",
    "                new_list.append(page+1)\n",
    "                break\n",
    "            if (page_list[i+1] - page) >= 4:\n",
    "                new_list.append(page-1)\n",
    "                new_list.append(page)\n",
    "                new_list.append(page+1)\n",
    "            else:\n",
    "                new_list.append(page-1)\n",
    "                for pgno in list(range(page,page_list[i+1])):\n",
    "                    new_list.append(pgno)\n",
    "        idx_pg_dict[key] = list(set([i for i in new_list if i != 0]))\n",
    "    if details == True:\n",
    "        for ID,pages in idx_pg_dict.items():\n",
    "            print(\"pages for {}:\".format(data_json[\"stored_pdfs\"][str(ID)]),pages)\n",
    "    final_list = []\n",
    "    for ID, pages in idx_pg_dict.items():\n",
    "        for page in pages:\n",
    "            final_list.append(get_page_text(ID,page))\n",
    "    return \" \".join(final_list)\n",
    "\n",
    "def chatbot(premise = \"Youâ€™re a technical support bot that has access to manuals. The manual information is here: \",\n",
    "            prompt_mod = \" Notes: You CAN ONLY USE INFO FROM THE MANUAL TEXT I REPEAT.  \\\n",
    "            You CAN ONLY USE INFO FROM THE MANUAL TEXT. If there is nothing available in \\\n",
    "            the text, SAY I don't have the info. Do not generalize answers. Do not state \\\n",
    "            the page numbers. Remember the answers can be from different manuals. Say \\\n",
    "            'According to my resources' if you understand this along with your response\",\n",
    "            \n",
    "            k_docs = 8,\n",
    "            k_pages = 10,\n",
    "            threshold = 0.7,\n",
    "            details = False):\n",
    "    '''Start of the chatbot. Able to modify the premise or prompt'''\n",
    "    #print(\"Key in 'new' to start a new conversation and 'exit' to end it\\n\")\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": premise}\n",
    "      ]\n",
    "    response_list = []\n",
    "    \n",
    "    \n",
    "    def conversation(response):\n",
    "        '''The openai api responses'''\n",
    "        if len(messages) == 1:\n",
    "            messages[0][\"content\"] += get_suitable_text(response,k_pages=k_pages,details=details)\n",
    "        response_list.append(response)\n",
    "        content = \"Question: \" + response + prompt_mod\n",
    "        messages.append({\"role\": \"user\", \"content\": content})\n",
    "        \n",
    "        completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k-0613\",\n",
    "        messages=messages,\n",
    "        )\n",
    "        message = completion[\"choices\"][0][\"message\"]\n",
    "        chat_response = completion.choices[0].message.content\n",
    "        token_usage = completion.usage.total_tokens\n",
    "        messages.append({\"role\": \"assistant\", \"content\": chat_response})\n",
    "\n",
    "        return (messages,token_usage)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    while True:\n",
    "        cur_msg = input(\"Question: \\n\")\n",
    "        if cur_msg == \"exit\":\n",
    "            clear_output()\n",
    "            break\n",
    "        elif cur_msg == \"manuals\":\n",
    "            with open(\"Prototype/data_json.json\", 'r') as openfile:\n",
    "                data_json = json.load(openfile)\n",
    "            for stored_pdf in list(data_json[\"stored_pdfs\"].values()):\n",
    "                print(stored_pdf)\n",
    "        elif cur_msg == \"load pdfs\":\n",
    "            load_pdfs()\n",
    "        elif cur_msg == \"load pdfs reset\":\n",
    "            load_pdfs(reset=True)\n",
    "        elif cur_msg == \"new\":\n",
    "            clear_output()\n",
    "            messages=[{\"role\": \"system\", \"content\": premise}]\n",
    "        else:\n",
    "            cur_conv = conversation(cur_msg)\n",
    "            print(\"\\nChatGPT:\\n\" + cur_conv[0][-1][\"content\"])\n",
    "            print(\"\\n\")\n",
    "            print(f\"tokens used: {cur_conv[1]}/16385\\n\")\n",
    "            continue\n",
    "    print(\"Chat Ended\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac65be4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tqdm' has no attribute 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9g/btnfbnzs5s98j60gmhcn4fnh0000gn/T/ipykernel_2339/2109992933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tqdm' has no attribute 'pandas'"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69260ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1499c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files detected.\n"
     ]
    }
   ],
   "source": [
    "# Add files to pdf_files before running to add new files\n",
    "# Set reset=True to reset the database and reload all embeddings\n",
    "load_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5daa9f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Ended\n"
     ]
    }
   ],
   "source": [
    "# Key in \"new\" to start a new context session\n",
    "# Key in \"exit\" to end the chatbot\n",
    "# Key in \"manuals\" to display all available manual\n",
    "# Key in \"load pdfs\" to load new pdfs or \"load pdfs reset\" to reset the database (costly) and load\n",
    "chatbot(details=True,k_pages=10,k_docs=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
