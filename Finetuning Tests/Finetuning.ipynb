{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75bf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355be9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_file = open(\"OPENAI_API_KEY\", \"r\")\n",
    "openai.api_key = api_key_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc871971",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Create training data in the form of prompt and responses.\n",
    "2. Put it into a csv with column names \"prompt\" and \"completion\" and save it.\n",
    "3. In CLI, run the data preprocessing step to convert the csv into a .jsonl file.\n",
    "4. You can now fine tune the model, and you can also choose a specific model you want to fine tune on.\n",
    "5. It might error and take a while if the .jsonl is large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b108daf",
   "metadata": {},
   "source": [
    "1. **Answering Frequently Asked Questions (FAQs)**: You can input a list of frequently asked questions and their corresponding answers into ChatGPT's training data. This way, when users ask common questions, ChatGPT can provide accurate and consistent responses.\n",
    "\n",
    "    This is largely possible depending on the size of the FAQ dataset. I can show a demo that clearly works. It worked on a UCI shrinked dataset.\n",
    "\n",
    " \n",
    "2. **Real-time Support**: ChatGPT can be integrated into websites or applications to offer real-time support to users. It can handle queries and respond instantly, providing a 24/7 support option.\n",
    "\n",
    "    This is also very possible, I already implemented something like this, but it is more for client side support vs internal use.\n",
    " \n",
    " \n",
    "3. **Troubleshooting and Issue Resolution**: Users can describe their problems or technical issues, and ChatGPT can guide them through the troubleshooting process step-by-step. It can offer solutions or suggest actions to resolve issues.\n",
    " \n",
    "    This would be possible for stuff available on the internet, but for propietary information, depending on how large the dataset is, is hard to do.\n",
    " \n",
    " \n",
    "4. **Product Recommendations**: By understanding user needs and preferences, ChatGPT can suggest relevant products or services based on the information provided.\n",
    "\n",
    "    Again, if the stuff wasn't propietary and already trained on the dataset, it is very possible.\n",
    " \n",
    " \n",
    "7. **Handling Complex Queries**: While simple questions are easy to manage, ChatGPT's ability to process and understand context allows it to handle more complex queries and offer appropriate responses.\n",
    "\n",
    "    This in my opinion is whats best about chatGPT, it can adapt to different scenarios with relative ease.\n",
    " \n",
    " \n",
    "9. **Personalization**: With enough user data and proper implementation, ChatGPT can offer personalized recommendations and responses, enhancing the overall support experience.\n",
    "\n",
    "    Enough data is the specificity of the data. There can also be too much data.\n",
    " \n",
    " \n",
    "10. **Feedback Collection**: ChatGPT can gather feedback from users about their experiences, which can be valuable for improving products or services.\n",
    "\n",
    "    This has been implemented by me. You can log information based on user feedback in you rown database (SQL).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e94d0",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/75266549/fine-tune-a-davinci-model-to-be-similar-to-instructgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72699d",
   "metadata": {},
   "source": [
    "Evaluation Steps\n",
    "\n",
    "I need to test openai on fictitious data to see if it can interpret data from a custom database\n",
    "\n",
    "Worked in a basic sense, but the performance is severely limited. It cannot infer intelligent connections and sometimes outputs jargon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('promptresponse.csv',delimiter='\\t').to_csv('processedprompts.csv',index=False)\n",
    "pd.DataFrame({\"prompt\":[],\"response\":[]})\n",
    "!openai tools fine_tunes.prepare_data -f 'processedprompts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8379c27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7gpXwpAOqiCoOzMOSX44KINUW6Mec at 0x7f86e0391ea0> JSON: {\n",
       "  \"id\": \"cmpl-7gpXwpAOqiCoOzMOSX44KINUW6Mec\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1690442276,\n",
       "  \"model\": \"davinci:ft-personal-2023-07-25-05-04-56\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" 802.11ac 802.11n 802.11ac 802.11ac 802.11ac 802.11ac 802.11ac 802.11ac 802.11ac 802.11ac 802.11ac Security Connectivity 2G/2.5G/3G/4G LTE/B1/B3/B7/B20 (CN) / B8/B28 (TW) WCDMA/GSM / DC-HSPA+ / TD-\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 4,\n",
       "    \"completion_tokens\": 100,\n",
       "    \"total_tokens\": 104\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "openai.Completion.create(\n",
    "    model=\"davinci:ft-personal-2023-07-25-05-04-56\",\n",
    "    prompt=\"802.11ah\",\n",
    "    max_tokens=100)#[\"choices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9acb05a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "train_data = []\n",
    "with open('uciprompts_prepared.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    train_data.append(result)\n",
    "    #print(f\"result: {result}\")\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a812309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: openai wandb [-h] {sync} ...\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help  show this help message and exit\r\n",
      "\r\n",
      "wandb:\r\n",
      "  {sync}      Logging with Weights & Biases\r\n"
     ]
    }
   ],
   "source": [
    "!openai wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5083f4da",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "No file with ID: uciprompts_prepared.jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9g/btnfbnzs5s98j60gmhcn4fnh0000gn/T/ipykernel_95980/2169608928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFineTune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"davinci\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"uciprompts_prepared.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/abstract/createable_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: No file with ID: uciprompts_prepared.jsonl"
     ]
    }
   ],
   "source": [
    "openai.FineTune.create(model=\"davinci\",training_file = \"uciprompts_prepared.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0d0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f4cbeca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7a2335a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c4b141cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mError:\u001b[0m No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!openai api fine_tunes.follow -i ft-hDG4o3RKxBeZ1GrpuuOHyDSg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
